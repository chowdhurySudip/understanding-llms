# Training Configuration for Transformer Language Model (LLM Intro)
# This file contains all hyperparameters and settings for training

# =============================================================================
# Data Configuration
# =============================================================================
data:
  train_path: "llm_intro/artifacts/TinyStoriesV2-GPT4-train-encoded.npy"
  val_path: "llm_intro/artifacts/TinyStoriesV2-GPT4-valid-encoded.npy"

# =============================================================================
# Training Hyperparameters
# =============================================================================
training:
  batch_size: 8          # Number of sequences per batch
  context_length: 256    # Maximum sequence length
  device: "mps"          # Device for training ("cpu", "cuda", or "mps")
  max_iteration: 100     # Reduced for testing
  val_batch_size: 128    # Batch size for validation
  val_interval: 20       # Evaluate on validation set every N iterations
  checkpoint_interval: 50  # Save checkpoint every N iterations
  checkpoint_dir: "llm_intro/artifacts/models"  # Directory to save checkpoints

# =============================================================================
# Model Architecture
# =============================================================================
model:
  vocab_size: 10000      # Size of vocabulary
  d_model: 64            # Embedding/hidden dimension
  num_layers: 2          # Number of transformer blocks
  num_heads: 2           # Number of attention heads per block
  d_ff: null             # Feedforward hidden dim (null = auto-compute)
  rope_theta: 10000      # RoPE base frequency parameter

# =============================================================================
# Optimizer Configuration
# =============================================================================
optimizer:
  type: "adamw"          # Optimizer type
  max_lr: 0.0006         # Maximum learning rate (6e-4)
  min_lr: 0.00006        # Minimum learning rate (6e-5)
  betas: [0.9, 0.999]    # Adam momentum parameters
  eps: 1.0e-6            # Adam epsilon for numerical stability
  weight_decay: 0.01     # Weight decay coefficient
  max_grad_norm: 1.0     # Maximum gradient norm for clipping

# =============================================================================
# Learning Rate Schedule
# =============================================================================
schedule:
  warmup_ratio: 0.1      # Warmup phase as fraction of total iterations
  cosine_ratio: 0.9      # Cosine annealing phase as fraction of total

# =============================================================================
# Plotting Configuration
# =============================================================================
plotting:
  enabled: false          # Disable plotting for headless/test runs
  style: "darkgrid"      # Seaborn style theme
  figsize: [15, 5]       # Figure size (width, height)

# =============================================================================
# Generation Configuration
# =============================================================================
generation:
  checkpoint_path: "llm_intro/artifacts/models/checkpoint_itr_50.pth" # Checkpoint to load (autogenerated name)
  vocab_path: "llm_intro/artifacts/TinyStoriesV2-GPT4-train-vocab.json"
  merges_path: "llm_intro/artifacts/TinyStoriesV2-GPT4-train-merges.txt"
  special_tokens: ["<|endoftext|>"]
  default_prompt: "Once upon a time"
  max_tokens: 50
  temperature: 0.8
  top_p: 0.9

